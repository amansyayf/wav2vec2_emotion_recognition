{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eEHCk9_uR0rUrSpEEU7J8hMwKcC_0_ig",
      "authorship_tag": "ABX9TyN13qW534Gzm5UYoVK2r2tt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "plCj37XauWy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zp8IzQzkOLeF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import HubertModel, Wav2Vec2FeatureExtractor, Wav2Vec2Tokenizer, Wav2Vec2Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from torchaudio import transforms\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQxHbOxuOxXZ",
        "outputId": "b0044587-b299-47d5-b771-124a4fa43c24"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AYMvGsRXVzhq",
        "outputId": "854d8d59-0dba-41e6-c568-28efa883b9bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "6jOEgtFjWEcT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_df(range, RAVDESS, dir_list):\n",
        "  emotion = []\n",
        "  path = []\n",
        "  for i in range:\n",
        "      directory = dir_list[i]\n",
        "      fname = os.listdir(RAVDESS + directory)\n",
        "      for f in fname:\n",
        "          part = f.split('.')[0].split('-')\n",
        "          emotion.append(int(part[2]))\n",
        "          path.append(RAVDESS + directory + '/' + f)\n",
        "\n",
        "  df = pd.DataFrame(emotion, columns=['label_class'])\n",
        "  df['label'] = df['label_class'].replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "  df = pd.concat([df, pd.DataFrame(path, columns=['path'])], axis=1)\n",
        "  df['label_class'] = df['label_class'] - 1\n",
        "  return df"
      ],
      "metadata": {
        "id": "9zY2yU0OPSrN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_dataset(path):\n",
        "  RAVDESS = path+'/'\n",
        "  dir_list = os.listdir(RAVDESS)\n",
        "  dir_list.sort()\n",
        "\n",
        "  # make train dataset\n",
        "  df_train = process_df(range(20), RAVDESS, dir_list)\n",
        "\n",
        "  # make val dataset\n",
        "  df_val = process_df(range(20, 22), RAVDESS, dir_list)\n",
        "\n",
        "  # make test dataset\n",
        "  df_test = process_df(range(22, 24), RAVDESS, dir_list)\n",
        "\n",
        "  return df_train, df_val, df_test\n"
      ],
      "metadata": {
        "id": "r1WhAhBNOmOK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = fetch_dataset(\"/content/drive/MyDrive/wav2vec/RAVDESS/\")"
      ],
      "metadata": {
        "id": "w2Cg4ib5WE8S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df, data_col, label_col, max_length=4*16000, new_sr=16000, use_aug=False):\n",
        "\n",
        "\n",
        "        self.file_path_list = df[data_col].tolist()\n",
        "        self.label_list = df[label_col].tolist()\n",
        "        self.max_length = max_length\n",
        "        self.new_sr = new_sr\n",
        "        self.use_aug = use_aug\n",
        "\n",
        "        total_len = len(self.file_path_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_path_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sample_rate = librosa.load(self.file_path_list[idx])\n",
        "        if sample_rate != self.new_sr:\n",
        "            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=self.new_sr)\n",
        "        label = self.label_list[idx]\n",
        "\n",
        "        # data augmentation\n",
        "        if self.use_aug:\n",
        "          if random.random()>0.8:\n",
        "              audio = self.noise(audio)\n",
        "          if random.random()>0.8:\n",
        "              audio = self.stretch(audio)\n",
        "\n",
        "        desired_length = self.max_length\n",
        "\n",
        "        # pad or trim the audio signal to the desired length\n",
        "        # pad the audio tensor with zeros to a fixed length of 160000\n",
        "        if len(audio) < desired_length:\n",
        "            padding = desired_length - len(audio)\n",
        "            audio = np.pad(audio, (0, padding), 'constant')\n",
        "        elif len(audio) > desired_length:\n",
        "            audio = audio[:desired_length]\n",
        "\n",
        "        return audio, label, self.file_path_list[idx]\n",
        "\n",
        "    def noise(self, data):\n",
        "      noise_amp = 0.01*np.random.uniform()*np.amax(data)\n",
        "      data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "      return data\n",
        "\n",
        "    def stretch(self, data):\n",
        "      rate = 1+0.1*np.random.uniform(-1, 1)\n",
        "      return librosa.effects.time_stretch(data, rate=rate)\n",
        "\n",
        "    def targets(self):\n",
        "        return  self.label_list"
      ],
      "metadata": {
        "id": "DH-De5mzRf5C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(df_train, df_val, df_test, BATCH_SIZE=8, use_aug = False):\n",
        "  if use_aug:\n",
        "    train_dataset = AudioDataset(df_train, 'path', 'label_class', use_aug=True)\n",
        "    class_count = Counter(train_dataset.targets())\n",
        "    class_weights = {i: 1/c for i, c in class_count.items()}\n",
        "    sample_weights = [0] * len(train_dataset)\n",
        "    for i, (data, label, file_path) in enumerate(tqdm(train_dataset)):\n",
        "        class_weight = class_weights[label]\n",
        "        sample_weights[i] = class_weight\n",
        "\n",
        "    N = int(3*max(class_count.values()) * len(class_count)/2)  # fit to 1.5*max\n",
        "    train_sampler = WeightedRandomSampler(sample_weights, num_samples=N, replacement=True)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=2)\n",
        "  else:\n",
        "    train_dataset = AudioDataset(df_train, 'path', 'label_class')\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "  val_dataset = AudioDataset(df_val, 'path', 'label_class')\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "  test_dataset = AudioDataset(df_test, 'path', 'label_class')\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "  dataloaders = {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
        "\n",
        "  return dataloaders"
      ],
      "metadata": {
        "id": "1nECzpEOSkxk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(path)\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(path)\n",
        "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "        self.conv1 = nn.Conv1d(199, 256, 1)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "        self.conv2 = nn.Conv1d(256, 1, 1)\n",
        "        self.fc1 = torch.nn.Linear(768, 256)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.fc2 = torch.nn.Linear(256, 8)\n",
        "\n",
        "\n",
        "    def forward(self, input, spec_aug=False, mixup_lambda=None):\n",
        "        input = self.feature_extractor(input, return_tensors=\"pt\", sampling_rate=16000).to(device)\n",
        "        input = input.input_values.squeeze(dim=0)\n",
        "        wav2feature = self.wav2vec2(input).last_hidden_state\n",
        "        # wav2feature = torch.mean(wav2feature, dim=1)\n",
        "        x = self.dropout1(F.relu(self.conv1(wav2feature)))\n",
        "        x = self.conv2(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.dropout2(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        x = torch.nn.functional.softmax(x, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "R_QqI8mtDc2P"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# facebook/wav2vec2-large-xlsr-53"
      ],
      "metadata": {
        "id": "og_IjtKxW7xo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer' : optimizer.state_dict()}\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print('model saved to %s' % checkpoint_path)\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    print('model loaded from %s' % checkpoint_path)"
      ],
      "metadata": {
        "id": "Pu3CsNNUepds"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner():\n",
        "  def __init__(self, model, opt, dataloaders, loss_fn, device, checkpoint_path):\n",
        "    self.model = model\n",
        "    self.opt = opt\n",
        "    self.data_loader = dataloaders\n",
        "    self.loss_fn = loss_fn\n",
        "    self.device = device\n",
        "    self.checkpoint_path = checkpoint_path\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    state = {\n",
        "        'state_dict': self.model.state_dict(),\n",
        "        'optimizer' : self.opt.state_dict()}\n",
        "    torch.save(state, self.checkpoint_path)\n",
        "    print('model saved to %s' % self.checkpoint_path)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "      state = torch.load(self.checkpoint_path)\n",
        "      self.model.load_state_dict(state['state_dict'])\n",
        "      self.opt.load_state_dict(state['optimizer'])\n",
        "      print('model loaded from %s' % self.checkpoint_path)\n",
        "\n",
        "  def accuracy_fn(self, y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred))\n",
        "    return acc\n",
        "\n",
        "  def train_step(self, train_losses = [], train_accuracies=[]):\n",
        "\n",
        "      train_loss, train_acc = 0, 0\n",
        "\n",
        "      self.model.train()\n",
        "\n",
        "      for batch, (X, y, file_path) in enumerate(self.data_loader['train']):\n",
        "          X, y = X.to(self.device), y.to(self.device)\n",
        "\n",
        "          y_prob = self.model(X).to(self.device)\n",
        "          y_pred = torch.argmax(y_prob, dim=1).to(self.device)\n",
        "\n",
        "          loss = self.loss_fn(torch.log(y_prob), y)\n",
        "          train_loss += loss\n",
        "          acc = self.accuracy_fn(y_true=y, y_pred=y_pred)\n",
        "          train_acc += acc\n",
        "\n",
        "          self.opt.zero_grad()\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      train_loss /= len(self.data_loader['train'])\n",
        "      train_acc /= len(self.data_loader['train'])\n",
        "\n",
        "      train_losses.append(train_loss.detach().cpu())\n",
        "      train_accuracies.append(train_acc)\n",
        "\n",
        "  def val_step(self, val_losses = [], val_accuracies = [], key='val'):\n",
        "\n",
        "      val_loss, val_acc = 0, 0\n",
        "\n",
        "      self.model.eval()\n",
        "\n",
        "      with torch.inference_mode():\n",
        "          for batch, (X, y, file_path) in enumerate(self.data_loader[key]):\n",
        "\n",
        "              X, y = X.to(self.device), y.to(self.device)\n",
        "\n",
        "              val_prob = self.model(X).to(self.device)\n",
        "              val_pred = torch.argmax(val_prob, dim=1).to(self.device)\n",
        "\n",
        "              loss = self.loss_fn(torch.log(val_prob), y)\n",
        "              val_loss += loss\n",
        "              acc = self.accuracy_fn(y_true=y, y_pred=val_pred)\n",
        "              val_acc += acc\n",
        "\n",
        "          val_loss /= len(self.data_loader[key])\n",
        "          val_acc /= len(self.data_loader[key])\n",
        "\n",
        "          if key == 'val':\n",
        "            if val_accuracies and val_acc > max(val_accuracies):\n",
        "              self.save_checkpoint()\n",
        "\n",
        "            val_losses.append(val_loss.detach().cpu())\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "          if key == 'test':\n",
        "            return {\"model_loss\": val_loss.detach().cpu(),\n",
        "                    \"model_acc\": val_acc}\n",
        "\n",
        "  def test(self):\n",
        "    if os.path.isfile(self.checkpoint_path):\n",
        "      self.load_checkpoint()\n",
        "    return self.val_step(key = 'test')\n",
        "\n",
        "  def fit(self, epochs = 15):\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        self.train_step(train_losses = train_losses, train_accuracies =train_accuracies)\n",
        "\n",
        "        self.val_step(val_losses = val_losses, val_accuracies = val_accuracies, key = 'val')\n",
        "\n",
        "        clear_output(True)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n",
        "\n",
        "        axes[0].set_title('loss')\n",
        "        axes[0].plot(train_losses, label='train')\n",
        "        axes[0].plot(val_losses, label='val')\n",
        "        axes[0].legend(loc='upper right')\n",
        "        axes[0].grid()\n",
        "\n",
        "        axes[1].set_title('acc')\n",
        "        axes[1].plot(train_accuracies, label='train')\n",
        "        axes[1].plot(val_accuracies, label='val')\n",
        "        axes[1].legend(loc='upper right')\n",
        "        axes[1].grid()\n",
        "\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "njbedZRahE9U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = get_dataloaders(df_train, df_val, df_test, use_aug = False)\n",
        "\n",
        "model = AudioClassifier(\"facebook/wav2vec2-base\").to(device)\n",
        "next(model.parameters()).device\n",
        "\n",
        "loss_fn = nn.NLLLoss() # Multi-category loss\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.005)\n",
        "checkpoint_path = '/content/drive/MyDrive/wav2vec/model1.pth'"
      ],
      "metadata": {
        "id": "dhp6k807uNGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner =  Learner(model, optimizer, dataloaders, loss_fn, device, checkpoint_path = checkpoint_path)\n",
        "learner.fit(epochs = 10)"
      ],
      "metadata": {
        "id": "6B6tfEMcuPnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learner.test()"
      ],
      "metadata": {
        "id": "q5HVL8FuuR7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}